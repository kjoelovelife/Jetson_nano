{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use model we trained to move jetBot smoothly on track. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that you have already downloaded ``best_steering_model_xy.pth`` and ``best_model.pth`` to work station as instructed in \"collision_avoidance/train_model.ipynb\" and \"Road_Following/train_model.ipynb\" notebook. Now, you should upload model file to JetBot in to this notebooks's directory. Once that's finished there should be a file named ``best_steering_model_xy.pth`` and ``best_model.pth`` in this notebook's directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Please make sure the file has uploaded fully before calling the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code below to initialize the PyTorch model. This should look very familiar from the training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "\n",
    "# road following \n",
    "model_road_following = torchvision.models.resnet18(pretrained=False)\n",
    "model_road_following.fc = torch.nn.Linear(512, 2)\n",
    "\n",
    "# collision avoidance\n",
    "item = ['free' , 'green_light' , 'red_light' , 'parking' , 'zebra' , 'fence_open' , 'fence_close' , 'duck']\n",
    "model_collision = torchvision.models.alexnet(pretrained=False)\n",
    "model_collision.classifier[6] = torch.nn.Linear(model_collision.classifier[6].in_features, len(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load the trained weights from the ``best_steering_model_xy.pth`` and ``best_model.pth`` file that you uploaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# road following\n",
    "model_road_following.load_state_dict(torch.load('best_steering_model_xy.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the model weights are located on the CPU memory execute the code below to transfer to the GPU device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "# collision avoidance\n",
    "model_collision = model_collision.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Pre-Processing Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For road following :\n",
    "We have now loaded our model, but there's a slight issue. The format that we trained our model doesnt exactly match the format of the camera. To do that, we need to do some preprocessing. This involves the following steps:\n",
    "\n",
    "1. Convert from HWC layout to CHW layout\n",
    "2. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\n",
    "3. Transfer the data from CPU memory to GPU memory\n",
    "4. Add a batch dimension\n",
    "\n",
    "For collision avoidance :\n",
    "We have now loaded our model, but there's a slight issue.  The format that we trained our model doesnt *exactly* match the format of the camera.  To do that, \n",
    "we need to do some *preprocessing*.  This involves the following steps\n",
    "\n",
    "1. Convert from BGR to RGB\n",
    "2. Convert from HWC layout to CHW layout\n",
    "3. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\n",
    "4. Transfer the data from CPU memory to GPU memory\n",
    "5. Add a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import cv2 , time\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "# road following\n",
    "road_following_mean = torch.Tensor([0.485, 0.456, 0.406]).cuda().half()\n",
    "road_following_std = torch.Tensor([0.229, 0.224, 0.225]).cuda().half()\n",
    "\n",
    "def road_following_preprocess(image):\n",
    "    image = PIL.Image.fromarray(image)\n",
    "    image = transforms.functional.to_tensor(image).to(device).half()\n",
    "    image.sub_(road_following_mean[:, None, None]).div_(road_following_std[:, None, None])\n",
    "    return image[None, ...]\n",
    "\n",
    "\n",
    "# collision avoidance\n",
    "collision_mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "collision_stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "collision_normalize = torchvision.transforms.Normalize(collision_mean, collision_stdev)\n",
    "\n",
    "def collision_preprocess(camera_value):\n",
    "    global device, collision_normalize\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = collision_normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! We've now defined our pre-processing function which can convert images from the camera format to the neural network input format.\n",
    "\n",
    "Now, let's start and display our camera.  You should be pretty familiar with this by now.  We'll also create a slider that will display the\n",
    "probability that the robot is blocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import ipywidgets\n",
    "import traitlets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "## configure cameara :  Camera.instance(capture_flip=2,width=224, height=224)\n",
    "camera = Camera.instance(capture_flip=2, width=224, height=224)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_widget = ipywidgets.Image(format='jpeg', width=224, height=224)\n",
    "\n",
    "#traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image_widget, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "slider = {}\n",
    "\n",
    "for text in item:\n",
    "    slider[text] = ipywidgets.widgets.FloatSlider(description=text, min=0.0, max=1.0, orientation='vertical')\n",
    "\n",
    "# If you have four serval items to do , please type :  display(widgets.HBox([image, slider[item[0]] , slider[item[1]] , slider[item[2]] , slider[item[3]] , ... ])) )\n",
    "# notice the start number is 0 .\n",
    "display(ipywidgets.widgets.HBox([image_widget, slider[item[0]] , slider[item[1]] , slider[item[2]] , slider[item[3]] , slider[item[4]] , slider[item[5]] , slider[item[6]] , slider[item[7]] ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll create a function that will get called whenever the camera's value changes. This function will do the following steps\n",
    "\n",
    "For road following : \n",
    "\n",
    "1. Pre-process the camera image\n",
    "2. Execute the neural network\n",
    "3. Compute the approximate steering value\n",
    "4. Control the motors using proportional / derivative control (PD)\n",
    "\n",
    "\n",
    "For collision avoidance :\n",
    "\n",
    "1. Pre-process the camera image\n",
    "2. Execute the neural network\n",
    "3. While the neural network output indicates we're blocked, we'll turn left, otherwise we go forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(change):\n",
    "    global  slider \n",
    "    image = change['new']\n",
    "    # collision avoidance\n",
    "    collision_x = collision_preprocess(image)\n",
    "    collision_y = model_collision(collision_x)\n",
    "\n",
    "    ## we apply the `softmax` function to normalize the output vector so it sums to 1 (which makes it a probability distribution)\n",
    "    collision_y = F.softmax(collision_y, dim=1)\n",
    "    confidence = {}\n",
    "    # item = ['free' , 'green_light' , 'red_light' , 'parking' , 'zebra' , 'fence_open' , 'fence_close' , 'duck']\n",
    "    for text in item:\n",
    "        confidence[text] = float(collision_y.flatten()[item.index(text)])\n",
    "        slider[text].value = confidence[text]\n",
    "        \n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We've created our neural network execution function, but now we need to attach it to the camera for processing.\n",
    "\n",
    "We accomplish that with the observe function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">WARNING: This code will move the robot!! Please make sure your robot has clearance and it is on Lego or Track you have collected data on. The road follower should work, but the neural network is only as good as the data it's trained on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! If your robot is plugged in it should now be generating new commands with each new camera frame. \n",
    "\n",
    "You can now place JetBot on  Lego or Track you have collected data on and see whether it can follow track.\n",
    "\n",
    "If you want to stop this behavior, you can unattach this callback by executing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "That's it for this live demo! Hopefully you had some fun seeing your JetBot moving smoothly on track follwing the road!!!\n",
    "\n",
    "If your JetBot wasn't following road very well, try to spot where it fails. The beauty is that we can collect more data for these failure scenarios and the JetBot should get even better :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
